---
layout: post
title: Optimizing Neural Network Learning with Clustering and Dimensional Reduction
permalink: /unsupervised_learning/
toc: true
---

## Overview

Unsupervised learning encompasses a wide range of techniques which aim to draw conclusions from unlabeled data, and each technique can have many approaches in itself. 

 - **Clustering** is a technique which attempts to partition data into groupings based on some notion of similarity; some clustering approaches include hierarchical clustering, *k*-means clustering, and Gaussian mixture models (GMM). 

 - **Dimensionality reduction** attempts to transform a set of features into a more simplified representation while preserving as much of the underlying signals in the original data as possible. Some approaches include random projection (RP), principal component analysis (PCA), and independent component analysis (ICA).

On this page, I will summarize my work and findings for the Unsupervised Learning assignment in Georgia Tech's CS 7641 course and discuss how clustering and dimensionality reduction can be useful for optimizing supervised learning. I chose to explore the [Dry Bean](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset){:target="_blank"} and [Estimation of Obesity Levels Based On Eating Habits and Physical Condition](https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition){:target="_blank"} ("Obesity") datasets from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu){:target="_blank"}. 

*(****Note:*** *Due to Georgia Institute of Technology's CS 7641 course policy, the GitHub repository for this work is private and available upon request.)*

## Datasets

The Dry Bean dataset is made up of continuous features extracted by a computer vision system from images of dry beans, and each instance is labeled with the varietal of dry bean. Supervised learning techniques are successful at learning this dataset—I achieved a test F1 score of 0.93 using scikit-learn's [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).

The Obesity dataset is made of continuous and categorical features derived from original and synthetic survey data, and each instance is labeled with the obesity level of the individual. Similarly, supervised learning of this dataset is successful—I achieved a test F1 score of 0.95 using scikit-learn's MLPClassifier.

## Clustering

Clustering on its own is useful for deriving meaningful groupings of instances in an unlabeled dataset. ***k*-means** performs **hard clustering** by minimizing squared distance between data points and their assigned centers, while **GMMs** perform **soft clustering** by maximizing the likelihood that data points were generated by their assigned Gaussian. GMMs are most commonly solved using expectation-maximization (EM) to maximize the likelihood of its parameters.

In addition, as I'll show here, it can be useful as a preprocessing stage for supervised learning of labeled datasets.

### Clustering Performance

First, let's visualize the effectiveness of clustering on the datasets (contingency matrices are also a useful tool but I did not include them here for brevity). For this, I used scikit-learn's [GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) and [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). The number of clusters for each algorithm and dataset were selected by analyzing the Calinski-Harabasz index and Davis-Bouldin index at different values of *k* (not shown here for brevity).

!["Dry Bean dataset clustering scatter plot"](/images/p1_drybean_clustering_scat.png "Dry Bean dataset clustering scatter plot"){:.project-image}

!["Obesity dataset clustering scatter plot"](/images/p1_obesity_clustering_scat.png "Obesity dataset clustering scatter plot"){:.project-image}

Both GMM and *k*-means clustering are much more effective on Dry Bean than Obesity. For Dry Bean, nearly all true classes in the dataset are partitioned, with the exception of true classes 0 and 2, which are grouped together. In contrast, only true class 6 can really be discerned in Obesity. These results are likely because Dry Bean's features are continuous and based on geometric properties, whereas Obesity's features are noisier and based on human survey responses.

### Cluster Labels as a New Input to Neural Networks

So how can clusters be used to optimize supervised learning? The approach I explored was to **use the labels generated through clustering as a *new input* to the corresponding neural network**. With Dry Bean, this meant generating a 17th feature using its existing 16 features. Obesity also happens to contain 16 features, which I used to generate a 17th.

Using five-fold cross validation, I performed a grid search to optimize the network architecture of the new, augmented, 17-input neural network models. I then tested the models on the 20% of the data I had held out for testing, yielding the following results (ignore the rows related to dimensionality reduction for now):

!["Neural network performance table"](/images/neuralnetwork_dimred_clustering.png "Neural network performance table"){:.project-image-large}

I was happy to find that **the optimal architectures of the augmented models were much simpler than those of the original models!** For Dry Bean, the number of hidden layers dropped from three to just one, and for Obesity, the number of hidden units in its single hidden layer dropped from 100 to just 9 and 7. 

Additionally, **the fit times of the augmented models were shorter than those of the original models!** For Dry Bean, fit time decreased by up to 39%, and for Obesity, it decreased by up to 35%.

However, F1 score tells us that not all datasets are made equal. As shown earlier, clustering was successful on Dry Bean but not on Obesity (where many true classes were misconstrued). It turns out that supervised learning using Obesity's inaccurate cluster data truly does impact prediction performance—the F1 scores of the augmented Obesity models were up to 27% worse. In contrast, Dry Bean's mostly accurate cluster data did *not* decrease its F1 score at all! From this, I concluded that **the Obesity dataset's low-quality unsupervised cluster labels negatively impacted the prediction performance of its augmented supervised learning models.**

## Dimensionality Reduction

Dimensionality reduction methods are used to transform feature sets into fewer, but more useful, components. **PCA** attempts to simplify a dataset by finding orthogonal components in order of how much variance they explain (i.e., how much reconstruction error they reduce). **RP** finds orthogonal components with less computational expense but less accuracy. In contrast to RP and PCA, **ICA** attempts separate data into the statistically independent, non-Gaussian causes underlying it. I like to think of RP and PCA as attempting to "summarize" the data and ICA as attempting to "decompose" it.

### Dimensionality Reduction Performance

Let's visualize how PCA and ICA transform the original features of the Dry Bean and Obesity datasets. For this, I used scikit-learn's [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html). The number of components for PCA was determined based on a threshold of 90 percent cumulative explained variance, and for ICA, components were selected by finding elbows in the kurtosis and reconstruction error plots (not shown here for brevity).

!["Dry Bean and Obesity dataset PCA projections"](/images/p2_pca_projections.png "Dry Bean and Obesity dataset PCA"){:.project-image-small}

!["Dry Bean and Obesity dataset ICA projections"](/images/p2_ica_projections.png "Dry Bean and Obesity dataset ICA projections"){:.project-image-small}

These plots visualize the raw value of each principal component (PC) and independent component (IC) for each instance in each data set. Note that these algorithms are unsupervised (blind to the true class labels) and that true class labels are superimposed for reference and analysis purposes. 

Each of Dry Bean's three PCs seem to "stratify" the class labels in some manner, while Obesity's eight PCs are less successful at doing so. Dry Bean's 6th and 7th IC seem to extract class labels 5 and 1, while Obesity's discrete features make it difficult to draw similar conclusions from its six ICs. Although I did not have time prior to the original assignment deadline, in the future it could be informative to plot PCs and ICs against each other to visualize the transformed data in two or three dimensions rather than just one.

### Neural Network Learning using Reduced Dimensions

How does dimensionality reduction impact neural network learning? Put differently, **how well can a neural network predict the class labels of the data set instances using the transformed, reduced feature spaces?** 

As with the clustering discussion above, I performed a grid search using five-fold cross validation to optimize the network architecture of the new neural network models. The test set results are as follows (this is the same table shown above):

!["Neural network performance table"](/images/neuralnetwork_dimred_clustering.png "Neural network performance table"){:.project-image-large}

**Dimensionality reduction improved neural network learning on the Dry Bean dataset**. RP, PCA, and ICA all resulted in simpler network architectures with up to 67% reduction in fit time. F1 score was unaffected except for PCA, which reduced F1 score by 5%. This was likely because only three princpal components were used; increasing this to four or even five could increase performance.

On the other hand, **dimensionality reduction harmed neural network learning for the Obesity dataset**. All three metrics—architecture complexity (except for RP), fit time, and F1 score—were worsened after RP, PCA, and ICA. Fit time increased by up to 139% and F1 score decreased by up to 26%.

One explanation for these results is that **dimensionality reduction preserved more information for the Dry Bean dataset than the Obesity dataset because Dry Bean's data represents continuous, sensor-based signals whereas Obesity's data is discrete and prone to biases in human survey responses**. Dry Bean's features may be more conducive to linear projection and reconstruction than Obesity's, and Obesity's human-based data may be non-Gaussian in so many dimensions that even ICA is unhelpful at reducing dimensionality.

\
Thanks for reading! Feel free to reach out on LinkedIn if you have questions or concerns. :)